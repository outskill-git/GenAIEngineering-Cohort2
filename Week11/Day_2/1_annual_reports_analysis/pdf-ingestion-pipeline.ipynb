{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Ingestion Pipeline with FAISS Vector Database\n",
    "\n",
    "This notebook processes PDF files from the `annual_reports` folder and creates a persistent FAISS index with support for both local models and HuggingFace inference endpoints.\n",
    "\n",
    "## ‚ö†Ô∏è Quick Start for Kernel Crash Issues\n",
    "\n",
    "If you're experiencing kernel crashes:\n",
    "1. First run the NumPy downgrade: `!pip install 'numpy<2' --force-reinstall`\n",
    "2. Set environment variables (see cell below)\n",
    "3. Use `use_safe_mode=True` when running the pipeline\n",
    "4. Reduce batch size or use the debug cells to identify issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured for stability\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Run this cell FIRST if experiencing crashes\n",
    "import os\n",
    "\n",
    "# Set environment variables for stability\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "print(\"‚úÖ Environment configured for stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies\n",
    "\n",
    "**Important**: If you encounter NumPy compatibility errors, run the NumPy downgrade command first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "print(f\"Current NumPy version: {np.__version__}\")\n",
    "\n",
    "# If you see version 2.x and get compatibility errors, uncomment the next line:\n",
    "# !pip install 'numpy<2' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Quick fix for NumPy compatibility (Recommended)\n",
    "# !pip install 'numpy<2' --force-reinstall\n",
    "\n",
    "# Option 2: Complete installation with compatible versions\n",
    "# !pip install 'numpy<2' faiss-cpu==1.7.4 'sentence-transformers>=2.2.0' 'transformers>=4.30.0' 'langchain>=0.0.200' PyPDF2 pdfplumber huggingface_hub python-dotenv tiktoken\n",
    "\n",
    "# Option 3: If you still have issues, create a fresh environment:\n",
    "# python -m venv pdf_rag_env\n",
    "# source pdf_rag_env/bin/activate  # On Windows: pdf_rag_env\\\\Scripts\\\\activate\n",
    "# pip install 'numpy<2' faiss-cpu sentence-transformers langchain PyPDF2 pdfplumber huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Check and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (main, Jul 25 2023, 17:07:07) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n",
      "Virtual environment: /Users/raamraam/outskill/GenAIEngineering-Cohort1/crewai/crewai_env\n",
      "‚úÖ NumPy 1.26.4\n"
     ]
    }
   ],
   "source": [
    "# Quick environment check\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Virtual environment: {sys.prefix}\")\n",
    "\n",
    "# Test critical imports\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"‚úÖ NumPy {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå NumPy import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Optional, Union\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PDF processing\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "\n",
    "# Text processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Vector store\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Utils\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "‚ö†Ô∏è **Memory Warning**: If processing large PDFs (>10MB) or many files, consider:\n",
    "- Reducing `CHUNK_SIZE` to 500\n",
    "- Processing files in batches\n",
    "- Using `use_safe_mode=True` in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuration loaded. PDF folder: annual_reports\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration for the ingestion pipeline\"\"\"\n",
    "    # Paths\n",
    "    PDF_FOLDER = \"annual_reports\"\n",
    "    FAISS_INDEX_PATH = \"faiss_index\"\n",
    "    METADATA_PATH = \"document_metadata.json\"\n",
    "    CHUNKS_PATH = \"document_chunks.pkl\"\n",
    "    \n",
    "    # Chunking parameters\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    \n",
    "    # Embedding models\n",
    "    DEFAULT_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    HF_API_EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    \n",
    "    # HuggingFace API (set your token as environment variable HF_TOKEN)\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "    \n",
    "    # FAISS parameters\n",
    "    EMBEDDING_DIM = 384  # Adjust based on your model\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(config.PDF_FOLDER, exist_ok=True)\n",
    "print(f\"üìÅ Configuration loaded. PDF folder: {config.PDF_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PDF Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PDF extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "def extract_text_pypdf2(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF using PyPDF2\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error with PyPDF2: {e}\")\n",
    "    return text\n",
    "\n",
    "def extract_text_pdfplumber(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF using pdfplumber (better for tables)\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error with pdfplumber: {e}\")\n",
    "    return text\n",
    "\n",
    "def extract_pdf_text(pdf_path: str, method: str = \"pdfplumber\") -> Dict:\n",
    "    \"\"\"Extract text and metadata from PDF\"\"\"\n",
    "    if method == \"pypdf2\":\n",
    "        text = extract_text_pypdf2(pdf_path)\n",
    "    else:\n",
    "        text = extract_text_pdfplumber(pdf_path)\n",
    "    \n",
    "    # Generate document ID\n",
    "    doc_id = hashlib.md5(pdf_path.encode()).hexdigest()\n",
    "    \n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"path\": pdf_path,\n",
    "        \"filename\": os.path.basename(pdf_path),\n",
    "        \"text\": text,\n",
    "        \"extraction_method\": method,\n",
    "        \"extraction_date\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ PDF extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text chunking functions defined\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Split text into chunks using RecursiveCharacterTextSplitter\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def create_document_chunks(documents: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Create chunks from documents with metadata\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = chunk_text(\n",
    "            doc[\"text\"], \n",
    "            chunk_size=config.CHUNK_SIZE, \n",
    "            chunk_overlap=config.CHUNK_OVERLAP\n",
    "        )\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_data = {\n",
    "                \"chunk_id\": f\"{doc['doc_id']}_{i}\",\n",
    "                \"doc_id\": doc[\"doc_id\"],\n",
    "                \"filename\": doc[\"filename\"],\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk,\n",
    "                \"char_count\": len(chunk)\n",
    "            }\n",
    "            all_chunks.append(chunk_data)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "print(\"‚úÖ Text chunking functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Debug Embedding Issues (Optional)\n",
    "\n",
    "If you're experiencing kernel crashes during embedding generation, run these debug cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available memory: 0.67 GB\n",
      "Total memory: 16.00 GB\n",
      "Memory usage: 95.8%\n",
      "\n",
      "PyTorch version: 2.2.2\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Debug cell - Check system resources\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"Installing psutil for system monitoring...\")\n",
    "    !pip install psutil\n",
    "    import psutil\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check available memory\n",
    "memory = psutil.virtual_memory()\n",
    "print(f\"Available memory: {memory.available / 1024**3:.2f} GB\")\n",
    "print(f\"Total memory: {memory.total / 1024**3:.2f} GB\")\n",
    "print(f\"Memory usage: {memory.percent}%\")\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test successful! Embeddings shape: (2, 384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative: Simple embedding generator for debugging\n",
    "def test_embedding_generation():\n",
    "    \"\"\"Test embedding generation with a small sample\"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    \n",
    "    # Force CPU usage\n",
    "    device = 'cpu'\n",
    "    torch.set_num_threads(1)  # Limit threads to avoid conflicts\n",
    "    \n",
    "    # Test with a tiny model first\n",
    "    test_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "    \n",
    "    # Test with small text\n",
    "    test_texts = [\"This is a test sentence.\", \"Another test.\"]\n",
    "    \n",
    "    try:\n",
    "        embeddings = test_model.encode(test_texts, batch_size=1)\n",
    "        print(f\"‚úÖ Test successful! Embeddings shape: {embeddings.shape}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_embedding_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Safe embedding function defined\n"
     ]
    }
   ],
   "source": [
    "# Alternative embedding function with maximum stability\n",
    "def generate_embeddings_safe(texts: List[str], model_name: str = 'all-MiniLM-L6-v2'):\n",
    "    \"\"\"Generate embeddings with maximum stability settings\"\"\"\n",
    "    import torch\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import gc\n",
    "    \n",
    "    # Configure for stability\n",
    "    torch.set_num_threads(1)\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    # Load model on CPU\n",
    "    model = SentenceTransformer(model_name, device='cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    batch_size = 4  # Very small batch size\n",
    "    \n",
    "    print(f\"Processing {len(texts)} texts in batches of {batch_size}...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        # Process batch\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode(\n",
    "                batch,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=False,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=False\n",
    "            )\n",
    "        \n",
    "        embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        # Aggressive memory cleanup\n",
    "        if i % 20 == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "print(\"‚úÖ Safe embedding function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding generator class defined with memory optimization\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, method: str = \"local\", model_name: str = None):\n",
    "        \"\"\"\n",
    "        Initialize embedding generator\n",
    "        Args:\n",
    "            method: 'local' or 'hf_api'\n",
    "            model_name: Model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.model_name = model_name or config.DEFAULT_EMBEDDING_MODEL\n",
    "        \n",
    "        if method == \"local\":\n",
    "            print(f\"Loading local model: {self.model_name}\")\n",
    "            # Set device to CPU to avoid CUDA issues\n",
    "            import torch\n",
    "            device = 'cpu'  # Force CPU to avoid GPU memory issues\n",
    "            print(f\"Using device: {device}\")\n",
    "            \n",
    "            self.model = SentenceTransformer(self.model_name, device=device)\n",
    "            self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "            \n",
    "            # Disable gradient computation to save memory\n",
    "            self.model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "            \n",
    "        elif method == \"hf_api\":\n",
    "            if not config.HF_TOKEN:\n",
    "                raise ValueError(\"HF_TOKEN not found. Set it as environment variable.\")\n",
    "            self.client = InferenceClient(token=config.HF_TOKEN)\n",
    "            self.model_name = config.HF_API_EMBEDDING_MODEL\n",
    "            # You'll need to know the embedding dimension for your API model\n",
    "            self.embedding_dim = 768  # for all-mpnet-base-v2\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str], batch_size: int = 8) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts\"\"\"\n",
    "        if self.method == \"local\":\n",
    "            return self._generate_local_embeddings(texts, batch_size)\n",
    "        elif self.method == \"hf_api\":\n",
    "            return self._generate_api_embeddings(texts)\n",
    "    \n",
    "    def _generate_local_embeddings(self, texts: List[str], batch_size: int) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings using local model with memory management\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        # Process in smaller batches to avoid memory issues\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Generate embeddings with error handling\n",
    "                batch_embeddings = self.model.encode(\n",
    "                    batch, \n",
    "                    convert_to_numpy=True,\n",
    "                    show_progress_bar=False,  # Disable nested progress bar\n",
    "                    batch_size=batch_size\n",
    "                )\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                \n",
    "                # Clear memory periodically\n",
    "                if i % (batch_size * 10) == 0:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i//batch_size}: {e}\")\n",
    "                # Create zero embeddings for failed batch\n",
    "                for _ in batch:\n",
    "                    embeddings.append(np.zeros(self.embedding_dim))\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def _generate_api_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings using HuggingFace API\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in tqdm(texts, desc=\"Generating embeddings via API\"):\n",
    "            try:\n",
    "                # Using feature extraction endpoint\n",
    "                embedding = self.client.feature_extraction(\n",
    "                    text,\n",
    "                    model=self.model_name\n",
    "                )\n",
    "                embeddings.append(embedding)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating embedding: {e}\")\n",
    "                # Fallback to zero vector\n",
    "                embeddings.append(np.zeros(self.embedding_dim))\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "\n",
    "print(\"‚úÖ Embedding generator class defined with memory optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FAISS Index Creation and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index manager class defined\n"
     ]
    }
   ],
   "source": [
    "class FAISSIndexManager:\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def create_index(self, embeddings: np.ndarray, chunks: List[Dict]):\n",
    "        \"\"\"Create FAISS index from embeddings\"\"\"\n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product for cosine similarity\n",
    "        self.index.add(embeddings)\n",
    "        \n",
    "        self.chunks = chunks\n",
    "        self.metadata = {\n",
    "            \"total_chunks\": len(chunks),\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"creation_date\": datetime.now().isoformat(),\n",
    "            \"documents\": list(set(chunk[\"filename\"] for chunk in chunks))\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Created FAISS index with {len(chunks)} chunks\")\n",
    "    \n",
    "    def save_index(self, base_path: str = None):\n",
    "        \"\"\"Save FAISS index and associated data\"\"\"\n",
    "        base_path = base_path or config.FAISS_INDEX_PATH\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, os.path.join(base_path, \"index.faiss\"))\n",
    "        \n",
    "        # Save chunks\n",
    "        with open(os.path.join(base_path, \"chunks.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.chunks, f)\n",
    "        \n",
    "        # Save metadata\n",
    "        with open(os.path.join(base_path, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Saved index to {base_path}\")\n",
    "    \n",
    "    def load_index(self, base_path: str = None):\n",
    "        \"\"\"Load FAISS index and associated data\"\"\"\n",
    "        base_path = base_path or config.FAISS_INDEX_PATH\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(os.path.join(base_path, \"index.faiss\"))\n",
    "        \n",
    "        # Load chunks\n",
    "        with open(os.path.join(base_path, \"chunks.pkl\"), \"rb\") as f:\n",
    "            self.chunks = pickle.load(f)\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(os.path.join(base_path, \"metadata.json\"), \"r\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        print(f\"üìÇ Loaded index from {base_path}\")\n",
    "        print(f\"   Total chunks: {self.metadata['total_chunks']}\")\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for similar chunks\"\"\"\n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Prepare results\n",
    "        results = []\n",
    "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            if idx < len(self.chunks):  # Valid index\n",
    "                result = {\n",
    "                    \"rank\": i + 1,\n",
    "                    \"score\": float(dist),\n",
    "                    \"chunk\": self.chunks[idx]\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ FAISS index manager class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ingestion_pipeline(\n",
    "    pdf_folder: str = None,\n",
    "    embedding_method: str = \"local\",\n",
    "    save_path: str = None,\n",
    "    use_safe_mode: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete ingestion pipeline\n",
    "    Args:\n",
    "        pdf_folder: Folder containing PDF files\n",
    "        embedding_method: 'local' or 'hf_api'\n",
    "        save_path: Path to save the FAISS index\n",
    "        use_safe_mode: Use safer embedding generation (slower but more stable)\n",
    "    \"\"\"\n",
    "    pdf_folder = pdf_folder or config.PDF_FOLDER\n",
    "    save_path = save_path or config.FAISS_INDEX_PATH\n",
    "    \n",
    "    print(f\"üöÄ Starting ingestion pipeline...\")\n",
    "    print(f\"   PDF folder: {pdf_folder}\")\n",
    "    print(f\"   Embedding method: {embedding_method}\")\n",
    "    print(f\"   Safe mode: {use_safe_mode}\")\n",
    "    \n",
    "    # Step 1: Extract text from PDFs\n",
    "    print(\"\\nüìÑ Step 1: Extracting text from PDFs...\")\n",
    "    documents = []\n",
    "    \n",
    "    pdf_files = list(Path(pdf_folder).glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise ValueError(f\"No PDF files found in {pdf_folder}\")\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"   Processing: {pdf_path.name}\")\n",
    "        doc_data = extract_pdf_text(str(pdf_path))\n",
    "        documents.append(doc_data)\n",
    "    \n",
    "    # Step 2: Create chunks\n",
    "    print(\"\\n‚úÇÔ∏è  Step 2: Creating document chunks...\")\n",
    "    chunks = create_document_chunks(documents)\n",
    "    print(f\"   Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "    \n",
    "    # Step 3: Generate embeddings\n",
    "    print(\"\\nüßÆ Step 3: Generating embeddings...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    \n",
    "    if use_safe_mode:\n",
    "        print(\"   Using safe mode for embedding generation...\")\n",
    "        embeddings = generate_embeddings_safe(chunk_texts)\n",
    "    else:\n",
    "        embedding_gen = EmbeddingGenerator(method=embedding_method)\n",
    "        embeddings = embedding_gen.generate_embeddings(chunk_texts, batch_size=8)\n",
    "    \n",
    "    print(f\"   Generated embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Step 4: Create and save FAISS index\n",
    "    print(\"\\nüóÇÔ∏è  Step 4: Creating FAISS index...\")\n",
    "    index_manager = FAISSIndexManager(embedding_dim=embeddings.shape[1])\n",
    "    index_manager.create_index(embeddings, chunks)\n",
    "    \n",
    "    # Step 5: Save everything\n",
    "    print(\"\\nüíæ Step 5: Saving index and metadata...\")\n",
    "    index_manager.save_index(save_path)\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "    return index_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run the Pipeline\n",
    "\n",
    "Make sure you have PDF files in the `annual_reports` folder before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting ingestion pipeline...\n",
      "   PDF folder: annual_reports\n",
      "   Embedding method: local\n",
      "   Safe mode: True\n",
      "\n",
      "üìÑ Step 1: Extracting text from PDFs...\n",
      "   Processing: NASDAQ_MSFT_2023.pdf\n",
      "   Processing: NASDAQ_AAPL_2023.pdf\n",
      "\n",
      "‚úÇÔ∏è  Step 2: Creating document chunks...\n",
      "   Created 682 chunks from 2 documents\n",
      "\n",
      "üßÆ Step 3: Generating embeddings...\n",
      "   Using safe mode for embedding generation...\n",
      "Processing 682 texts in batches of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [02:25<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generated embeddings with shape: (682, 384)\n",
      "\n",
      "üóÇÔ∏è  Step 4: Creating FAISS index...\n",
      "‚úÖ Created FAISS index with 682 chunks\n",
      "\n",
      "üíæ Step 5: Saving index and metadata...\n",
      "üíæ Saved index to faiss_index\n",
      "\n",
      "‚úÖ Pipeline completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Run with normal mode (may crash on some systems)\n",
    "# index_manager = run_ingestion_pipeline(\n",
    "#     pdf_folder=\"annual_reports\",\n",
    "#     embedding_method=\"local\",\n",
    "#     save_path=\"faiss_index\"\n",
    "# )\n",
    "\n",
    "# Option 2: Run with safe mode (recommended if kernel crashes)\n",
    "index_manager = run_ingestion_pipeline(\n",
    "    pdf_folder=\"annual_reports\",\n",
    "    embedding_method=\"local\",\n",
    "    save_path=\"faiss_index\",\n",
    "    use_safe_mode=True  # Enable safe mode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5. Troubleshooting Kernel Crashes\n",
    "\n",
    "If you're still experiencing kernel crashes, try these solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables set for stability\n"
     ]
    }
   ],
   "source": [
    "# Solution 1: Install/reinstall critical packages with specific versions\n",
    "# !pip install --upgrade --force-reinstall 'numpy<2' torch==2.0.1 sentence-transformers==2.2.2\n",
    "\n",
    "# Solution 2: Set environment variables before importing\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Solution 3: Clear all variables and restart\n",
    "# %reset -f\n",
    "# Then restart kernel and run cells again\n",
    "\n",
    "# Solution 4: As a last resort, restart Jupyter completely\n",
    "# Close this notebook, restart Jupyter server, and try again\n",
    "\n",
    "print(\"Environment variables set for stability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Process PDFs one at a time to isolate issues\n",
    "def process_single_pdf(pdf_path: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "    \"\"\"Process a single PDF file - useful for debugging\"\"\"\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    \n",
    "    # Extract text\n",
    "    doc_data = extract_pdf_text(pdf_path)\n",
    "    print(f\"  Extracted {len(doc_data['text'])} characters\")\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = create_document_chunks([doc_data])\n",
    "    print(f\"  Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Generate embeddings safely\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    embeddings = generate_embeddings_safe(chunk_texts, model_name)\n",
    "    print(f\"  Generated embeddings: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings, chunks\n",
    "\n",
    "# Test with a single PDF\n",
    "# pdf_files = list(Path(\"annual_reports\").glob(\"*.pdf\"))\n",
    "# if pdf_files:\n",
    "#     embeddings, chunks = process_single_pdf(str(pdf_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_search(query: str, k: int = 5, embedding_method: str = \"local\"):\n",
    "    \"\"\"Quick search function after index is created\"\"\"\n",
    "    # Load existing index\n",
    "    index_manager = FAISSIndexManager(embedding_dim=384)  # Adjust based on your model\n",
    "    index_manager.load_index(\"faiss_index\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    embedding_gen = EmbeddingGenerator(method=embedding_method)\n",
    "    query_embedding = embedding_gen.generate_embeddings([query])\n",
    "    \n",
    "    # Search\n",
    "    results = index_manager.search(query_embedding[0], k=k)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüîç Query: {query}\")\n",
    "    print(f\"üìä Top {k} results:\")\n",
    "    for result in results:\n",
    "        print(f\"\\n[Rank {result['rank']}] Score: {result['score']:.4f}\")\n",
    "        print(f\"üìÑ Document: {result['chunk']['filename']}\")\n",
    "        print(f\"üìù Text: {result['chunk']['text'][:200]}...\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Example Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded index from faiss_index\n",
      "   Total chunks: 682\n",
      "Loading local model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Query: revenue growth\n",
      "üìä Top 3 results:\n",
      "\n",
      "[Rank 1] Score: 0.5754\n",
      "üìÑ Document: NASDAQ_MSFT_2023.pdf\n",
      "üìù Text: and fourth quarter revenue is driven by the volume of multi-year on-premises contracts executed during the period.\n",
      "Change in Accounting Estimate\n",
      "In July 2022, we completed an assessment of the useful ...\n",
      "\n",
      "[Rank 2] Score: 0.5710\n",
      "üìÑ Document: NASDAQ_MSFT_2023.pdf\n",
      "üìù Text: SEGMENT RESULTS OF OPERATIONS\n",
      "Percentage\n",
      "(In millions, except percentages) 2023 2022 Change\n",
      "R evenue\n",
      "P roductivity and Business Processes $ 69,274 $ 63,364 9%\n",
      "Intelligent Cloud 87,907 74,965 17%\n",
      "More ...\n",
      "\n",
      "[Rank 3] Score: 0.5507\n",
      "üìÑ Document: NASDAQ_MSFT_2023.pdf\n",
      "üìù Text: in accounting estimate, gross margin percentage increased slightly driven by improvement in Office 365\n",
      "Commercial, offset in part by sales mix shift to cloud offerings.\n",
      "‚Ä¢ Operating expenses increased ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example search - modify the query based on your documents\n",
    "results = quick_search(\"revenue growth\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_index_with_new_pdfs(new_pdf_paths: List[str], existing_index_path: str = \"faiss_index\"):\n",
    "    \"\"\"Add new PDFs to existing index\"\"\"\n",
    "    # Load existing index and data\n",
    "    index_manager = FAISSIndexManager(embedding_dim=384)\n",
    "    index_manager.load_index(existing_index_path)\n",
    "    \n",
    "    # Process new PDFs\n",
    "    new_documents = []\n",
    "    for pdf_path in new_pdf_paths:\n",
    "        doc_data = extract_pdf_text(pdf_path)\n",
    "        new_documents.append(doc_data)\n",
    "    \n",
    "    # Create chunks for new documents\n",
    "    new_chunks = create_document_chunks(new_documents)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embedding_gen = EmbeddingGenerator(method=\"local\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in new_chunks]\n",
    "    new_embeddings = embedding_gen.generate_embeddings(chunk_texts)\n",
    "    \n",
    "    # Normalize and add to index\n",
    "    faiss.normalize_L2(new_embeddings)\n",
    "    index_manager.index.add(new_embeddings)\n",
    "    \n",
    "    # Update chunks and metadata\n",
    "    index_manager.chunks.extend(new_chunks)\n",
    "    index_manager.metadata[\"total_chunks\"] = len(index_manager.chunks)\n",
    "    index_manager.metadata[\"documents\"].extend([chunk[\"filename\"] for chunk in new_chunks])\n",
    "    index_manager.metadata[\"last_updated\"] = datetime.now().isoformat()\n",
    "    \n",
    "    # Save updated index\n",
    "    index_manager.save_index(existing_index_path)\n",
    "    \n",
    "    print(f\"‚úÖ Added {len(new_chunks)} chunks from {len(new_documents)} new documents\")\n",
    "\n",
    "def get_index_stats(index_path: str = \"faiss_index\"):\n",
    "    \"\"\"Get statistics about the index\"\"\"\n",
    "    with open(os.path.join(index_path, \"metadata.json\"), \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(\"üìä FAISS Index Statistics:\")\n",
    "    print(f\"   - Total chunks: {metadata['total_chunks']}\")\n",
    "    print(f\"   - Embedding dimension: {metadata['embedding_dim']}\")\n",
    "    print(f\"   - Documents indexed: {len(metadata['documents'])}\")\n",
    "    print(f\"   - Creation date: {metadata['creation_date']}\")\n",
    "    print(f\"   - Documents: {', '.join(metadata['documents'])}\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Get Index Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä FAISS Index Statistics:\n",
      "   - Total chunks: 682\n",
      "   - Embedding dimension: 384\n",
      "   - Documents indexed: 2\n",
      "   - Creation date: 2025-07-11T12:20:20.038881\n",
      "   - Documents: NASDAQ_AAPL_2023.pdf, NASDAQ_MSFT_2023.pdf\n"
     ]
    }
   ],
   "source": [
    "# Get statistics about your index\n",
    "stats = get_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Alternative: Minimal Processing (If All Else Fails)\n",
    "\n",
    "If you're still experiencing crashes, here's a minimal approach that processes one file at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal pipeline for problematic systems\n",
    "def minimal_pipeline():\n",
    "    \"\"\"Ultra-safe minimal pipeline\"\"\"\n",
    "    import gc\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # Use the lightest model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "    \n",
    "    # Process each PDF separately\n",
    "    pdf_files = list(Path(\"annual_reports\").glob(\"*.pdf\"))\n",
    "    all_embeddings = []\n",
    "    all_chunks = []\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing {pdf_file.name}...\")\n",
    "        \n",
    "        # Extract text\n",
    "        doc = extract_pdf_text(str(pdf_file), method=\"pypdf2\")\n",
    "        \n",
    "        # Simple chunking\n",
    "        text = doc['text']\n",
    "        chunk_size = 500\n",
    "        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-50)]\n",
    "        \n",
    "        # Process in tiny batches\n",
    "        for i in range(0, len(chunks), 2):\n",
    "            batch = chunks[i:i+2]\n",
    "            embeddings = model.encode(batch, convert_to_numpy=True)\n",
    "            all_embeddings.extend(embeddings)\n",
    "            \n",
    "            # Create chunk metadata\n",
    "            for j, chunk_text in enumerate(batch):\n",
    "                all_chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"filename\": pdf_file.name,\n",
    "                    \"chunk_index\": i + j\n",
    "                })\n",
    "        \n",
    "        # Aggressive cleanup\n",
    "        gc.collect()\n",
    "        print(f\"  Processed {len(chunks)} chunks\")\n",
    "    \n",
    "    # Create simple FAISS index\n",
    "    embeddings_array = np.array(all_embeddings)\n",
    "    index = faiss.IndexFlatL2(embeddings_array.shape[1])\n",
    "    index.add(embeddings_array)\n",
    "    \n",
    "    # Save\n",
    "    os.makedirs(\"faiss_index_minimal\", exist_ok=True)\n",
    "    faiss.write_index(index, \"faiss_index_minimal/index.faiss\")\n",
    "    \n",
    "    with open(\"faiss_index_minimal/chunks.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Minimal pipeline complete. Processed {len(all_chunks)} chunks.\")\n",
    "    return index, all_chunks\n",
    "\n",
    "# Run minimal pipeline\n",
    "# index, chunks = minimal_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
